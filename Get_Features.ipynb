{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4/GvDqHw3IBxGv5QIlvFt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManelSoengas/tutorials/blob/main/Get_Features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<font color='brown'>Obtención de las características</font>**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "El objetivo de técnicas como TF-IDF, Bag-of-Words (BOW), One-Hot Encoding, y Word Embeddings es transformar texto (palabras o frases) en una representación numérica (características) que los algoritmos de machine learning puedan procesar. Estas representaciones permiten al modelo entender el texto de manera que pueda identificar relaciones y patrones entre los datos (texto). Cada técnica captura diferentes aspectos del texto: frecuencia de palabras, contexto, y similitudes semánticas, lo que ayuda al modelo a aprender de los datos textuales y hacer predicciones o clasificaciones."
      ],
      "metadata": {
        "id": "TOgIw5Ga-O2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "le7Z8BGY3YDm",
        "outputId": "e550ddc8-7cee-456b-a773-532e276ab366"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.8.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<font color='brown'>BOW</font>**\n",
        "\n",
        "---\n",
        "Bag-of-Words (BOW): Convierte el texto en una matriz de recuento de palabras.\n"
      ],
      "metadata": {
        "id": "lP7c10Bc-dAC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obprJYQd1kEy",
        "outputId": "b8969de2-8dcb-4fe5-c30d-5564b896f4dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words (BOW):\n",
            " [[0 0 0 0 1 1 1 0 0 0 0]\n",
            " [1 0 0 1 0 0 0 0 1 0 0]\n",
            " [0 1 1 0 0 1 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0 1 0 1 1]]\n",
            "Vocabulary:\n",
            " ['amazing' 'coding' 'in' 'is' 'learning' 'love' 'machine' 'perro' 'python'\n",
            " 'tengo' 'un']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Datos de ejemplo\n",
        "corpus = [\"I love machine learning\", \"Python is amazing\", \"I love coding in Python\", \"Yo tengo un perro\"]\n",
        "\n",
        "# Definición de un vocabulario de palabras a eliminar del tokenizado. No aportan valor.\n",
        "\n",
        "stop_words = [\"i\", \"me\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\",\n",
        "              \"yo\", \"tú\", \"él\", \"ella\", \"nosotros\", \"vosotros\", \"ellos\"]\n",
        "\n",
        "# Bag-of-Words\n",
        "vectorizer = CountVectorizer(stop_words=stop_words)\n",
        "X_bow = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"Bag of Words (BOW):\\n\", X_bow.toarray())\n",
        "print(\"Vocabulary:\\n\", vectorizer.get_feature_names_out())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al utilizar el enfoque de Bag-of-Words (BOW) con CountVectorizer, se realiza un preprocesado básico del texto. Este preprocesado puede incluir la eliminación de palabras comunes o \"stopwords\" (como pronombres, artículos, preposiciones, etc.) que son consideradas poco informativas para el modelo. Este preprocesado puede variar en función del idioma. Para evitar estas diferencias un solución consite en definir un diccionario de palabras que no deben ser tenidas en cuenta en el moemnto de tokenizar (stopwords).\n",
        "\n",
        "**Preprocesado**: El paso previo a la obtención de las carcaterísticas es el preprocesado del texto de entrada. Como ejemplo existe la tarea de análisis de sentimientos, donde el proprocesado connsiste en:\n",
        "\n",
        "1. Eliminar stopwords.\n",
        "2. Convertir a minúsculas.\n",
        "3. Eliminar signos de puntuación.\n",
        "4. Lematización o stemming.\n",
        "5. Tokenización y Vectorización:\n",
        "\n",
        "En función del idioma, la calidad y el tipo de texto, el preprocesado puede requerir más operaciones, como por ejemplo, eliminar acentos, eliminar nicknames,eliminar slang, etc.\n",
        "Este enfoque garantiza que el texto esté en la mejor forma posible antes de ser vectorizado."
      ],
      "metadata": {
        "id": "HRpPY2fm2a5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<font color='brown'>ONE-HOT Encoding</font>**\n",
        "\n",
        "\n",
        "---\n",
        "Cada palabra es codificada en un vector binario.\n"
      ],
      "metadata": {
        "id": "WPReDWx-ogS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Tokenizar y convertir a one-hot\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "X_one_hot = tokenizer.texts_to_matrix(corpus, mode='binary')\n",
        "\n",
        "print(\"One-Hot Encoding:\\n\", X_one_hot)\n",
        "print(\"Word Index:\\n\", tokenizer.word_index)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzNOlO2U16Vf",
        "outputId": "c6981dc1-5191-4f81-da28-9c6d71e0c0f1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-Hot Encoding:\n",
            " [[0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]]\n",
            "Word Index:\n",
            " {'i': 1, 'love': 2, 'python': 3, 'machine': 4, 'learning': 5, 'is': 6, 'amazing': 7, 'coding': 8, 'in': 9, 'yo': 10, 'tengo': 11, 'un': 12, 'perro': 13}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El enfoque de One-Hot Encoding,Tokenizer de Keras, no realiza un preprocesado completo, pero sí incluye algunas operaciones básicas:\n",
        "\n",
        "1. **Tokenización**: Divide el texto en palabras o \"tokens\".\n",
        "2. **Normalización**: Convierte todo a minúsculas de forma predeterminada.\n",
        "3. **Asignación de índices**: Asigna un índice numérico a cada palabra en función de su frecuencia o aparición.\n",
        "4. **No elimina stopwords** ni realiza lematización o stemming automáticamente. Por lo tanto, si necesitas un preprocesado más avanzado, como eliminar stopwords o normalizar el texto, deberías hacerlo antes de aplicar el Tokenizer."
      ],
      "metadata": {
        "id": "eulGryhe4cqv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<font color='brown'>Word Embeddings</font>**\n",
        "\n",
        "---\n",
        "\n",
        "Se crean representaciones densas (embeddings) de las palabras, permitiendo capturar la similitud semántica. **Word2vec** es una técnica o modelo para representar palabras en un espacio vectorial, donde palabras con significados similares tienen vectores cercanos. La similitud entre dos palabras en **Word2Vec** se calcula típicamente utilizando la similitud del coseno. La similitud del coseno mide el ángulo entre los vectores de las dos palabras en el espacio vectorial.\n",
        "\n",
        "Los valores del **embedding** para \"Python\" obtemidos en el ejemplo son los componentes del vector que **Word2Vec** ha aprendido durante el entrenamiento.\n",
        "\n",
        "**Word Embeddings** capturan de manera más efectiva las características de las palabras, especialmente las relaciones semánticas y contextuales.\n"
      ],
      "metadata": {
        "id": "WID6l_QSsDAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Datos de ejemplo\n",
        "sentences = [[\"I\", \"love\", \"machine\", \"learning\"],\n",
        "             [\"Python\", \"is\", \"amazing\"],\n",
        "             [\"I\", \"love\", \"coding\", \"in\", \"Python\"]]\n",
        "\n",
        "# Entrenar el modelo Word2Vec\n",
        "model = Word2Vec(sentences, vector_size=12, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Obtener el embedding de una palabra\n",
        "embedding = model.wv['Python']\n",
        "print(\"Word Embedding for 'Python':\\n\", embedding)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiP3UHky5gVm",
        "outputId": "582a652c-ebe1-4780-92cc-e85727e1c9ee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Embedding for 'Python':\n",
            " [-0.00446856  0.00197026  0.04252791  0.07507727 -0.07752458 -0.05930674\n",
            "  0.05382394  0.0747749  -0.04179524 -0.03136143  0.0615042  -0.01277893]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Similaridad entre 'Python' y 'machine'\n",
        "similarity_python_machine = model.wv.similarity('Python', 'machine')\n",
        "print(\"Similaridad entre 'Python' y 'machine':\", similarity_python_machine)\n",
        "\n",
        "# Similaridad entre 'Python' y 'love'\n",
        "similarity_python_love = model.wv.similarity('Python', 'love')\n",
        "print(\"Similaridad entre 'Python' y 'love':\", similarity_python_love)\n",
        "\n",
        "# Embeddings de las palabras 'Python', 'machine' y 'love'\n",
        "embedding_python = model.wv['Python']\n",
        "embedding_machine = model.wv['machine']\n",
        "embedding_love = model.wv['love']\n",
        "\n",
        "print(\"\\nEmbedding for 'Python':\\n\", embedding_python)\n",
        "print(\"\\nEmbedding for 'machine':\\n\", embedding_machine)\n",
        "print(\"\\nEmbedding for 'love':\\n\", embedding_love)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cVZ8v_yvt76",
        "outputId": "fbc0fb24-34cd-4d26-942c-fed6ef2c12b2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similaridad entre 'Python' y 'machine': 0.41422442\n",
            "Similaridad entre 'Python' y 'love': -0.5136541\n",
            "\n",
            "Embedding for 'Python':\n",
            " [-0.00446856  0.00197026  0.04252791  0.07507727 -0.07752458 -0.05930674\n",
            "  0.05382394  0.0747749  -0.04179524 -0.03136143  0.0615042  -0.01277893]\n",
            "\n",
            "Embedding for 'machine':\n",
            " [-0.07431158 -0.05867967  0.00751213  0.05327111 -0.07183073  0.03054782\n",
            "  0.04324903  0.04784949  0.06222432 -0.05139729  0.00921345  0.05039402]\n",
            "\n",
            "Embedding for 'love':\n",
            " [-0.03780511  0.0546171  -0.04050134 -0.01513348  0.0239715   0.00826561\n",
            " -0.06904346 -0.07874015  0.06093138  0.04225218  0.05631411  0.00635721]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **vector_size**=12, indica el tamaño del vector obtenido.Si el corpus es pequeño valores entre 50 y 100. Si el corpus es grande se puede llegar a 500. En todo caso hay que probar diferentes valores y observar el que mejor captura el contexto.\n",
        "2. **window**=5, tiene en cuente el número de palabras a al izq. y drcha de la palabra objetivo. Con ello definimos el tamaño del contexto. Relacionado con el tipo de texto."
      ],
      "metadata": {
        "id": "_Fe0Q1KrxsH2"
      }
    }
  ]
}